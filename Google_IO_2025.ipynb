{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qSgfceeOAG9"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "# APIキーの設定\n",
        "try:\n",
        "    GOOGLE_API_KEY: str = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"シークレット 'GOOGLE_API_KEY' が見つかりません。ColabのシークレットマネージャーにAPIキーを設定してください。\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"APIキーの設定中にエラーが発生しました: {e}\")\n",
        "    exit()\n",
        "\n",
        "def query_model(model_name: str, prompt: str) -> bool:\n",
        "    \"\"\"\n",
        "    指定されたモデルにプロンプトを送信し、結果と統計情報を表示します。\n",
        "\n",
        "    Args:\n",
        "        model_name (str): 使用するGenerative AIモデルの名前\n",
        "        prompt (str): モデルに送信するプロンプトテキスト\n",
        "\n",
        "    Returns:\n",
        "        bool: モデルの実行が成功した場合True、失敗した場合False\n",
        "\n",
        "    Raises:\n",
        "        Exception: モデルの生成処理中にエラーが発生した場合\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"LLMモデル: {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    start_time: float\n",
        "    generation_time: float\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(model_name)\n",
        "\n",
        "        # 生成開始時刻を記録\n",
        "        start_time = time.time()\n",
        "        start_datetime: datetime = datetime.now()\n",
        "\n",
        "        print(f\"生成開始時刻: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(\"生成中...\")\n",
        "\n",
        "        # コンテンツ生成\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        # 生成終了時刻を記録\n",
        "        end_time: float = time.time()\n",
        "        end_datetime: datetime = datetime.now()\n",
        "        generation_time = end_time - start_time\n",
        "\n",
        "        # 結果表示\n",
        "        print(f\"\\n生成結果:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(response.text)\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # 統計情報表示\n",
        "        print(f\"\\n生成統計:\")\n",
        "        print(f\"   生成時間: {generation_time:.2f} 秒\")\n",
        "        print(f\"   完了時刻: {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "        # レスポンスオブジェクトから追加情報を取得（利用可能な場合）\n",
        "        if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
        "            usage = response.usage_metadata\n",
        "            prompt_tokens: Optional[int] = getattr(usage, 'prompt_token_count', None)\n",
        "            candidates_tokens: Optional[int] = getattr(usage, 'candidates_token_count', None)\n",
        "            total_tokens: Optional[int] = getattr(usage, 'total_token_count', None)\n",
        "\n",
        "            print(f\"   プロンプトトークン数: {prompt_tokens if prompt_tokens is not None else 'N/A'}\")\n",
        "            print(f\"   生成トークン数: {candidates_tokens if candidates_tokens is not None else 'N/A'}\")\n",
        "            print(f\"   総トークン数: {total_tokens if total_tokens is not None else 'N/A'}\")\n",
        "\n",
        "        # 文字数統計\n",
        "        response_length: int = len(response.text)\n",
        "        words_count: int = len(response.text.split())\n",
        "        print(f\"   生成文字数: {response_length} 文字\")\n",
        "        print(f\"   生成単語数: {words_count} 語\")\n",
        "\n",
        "        # 生成速度計算\n",
        "        if generation_time > 0:\n",
        "            chars_per_second: float = response_length / generation_time\n",
        "            print(f\"   生成速度: {chars_per_second:.1f} 文字/秒\")\n",
        "\n",
        "        print(f\"   ステータス: 成功\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        end_time = time.time()\n",
        "        generation_time = end_time - start_time if 'start_time' in locals() else 0\n",
        "\n",
        "        print(f\"\\nエラーが発生しました:\")\n",
        "        print(f\"   エラー内容: {e}\")\n",
        "        print(f\"   エラーまでの時間: {generation_time:.2f} 秒\")\n",
        "        print(f\"   ステータス: 失敗\")\n",
        "        return False\n",
        "\n",
        "    finally:\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "def main() -> None:\n",
        "    \"\"\"\n",
        "    複数のモデルでテストを実行し、全体の統計結果を表示します。\n",
        "\n",
        "    設定されたモデルリストに対して順次テストを実行し、\n",
        "    各モデルの結果と全体のサマリー統計を出力します。\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Raises:\n",
        "        Exception: APIキーの設定やモデルの実行中にエラーが発生した場合\n",
        "    \"\"\"\n",
        "    models_to_test: List[str] = [\n",
        "        \"gemini-2.5-flash-preview-05-20\",\n",
        "        \"gemma-3n-e4b-it\"\n",
        "    ]\n",
        "\n",
        "    prompt_text: str = \"真空中でもなぜ熱は伝わるのでしょうか？簡潔に説明してください。\"\n",
        "\n",
        "    print(f\"Google Generative AI モデル比較テスト\")\n",
        "    print(f\"プロンプト: \\\"{prompt_text}\\\"\")\n",
        "    print(f\"テスト対象モデル数: {len(models_to_test)}\")\n",
        "    print(f\"開始時刻: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    overall_start_time: float = time.time()\n",
        "    successful_models: int = 0\n",
        "    failed_models: int = 0\n",
        "\n",
        "    for i, model_id in enumerate(models_to_test, 1):\n",
        "        print(f\"\\n{i}/{len(models_to_test)} 番目のモデルをテスト中...\")\n",
        "        try:\n",
        "            success: bool = query_model(model_id, prompt_text)\n",
        "            if success:\n",
        "                successful_models += 1\n",
        "            else:\n",
        "                failed_models += 1\n",
        "        except Exception as e:\n",
        "            print(f\"モデル {model_id} でエラー: {e}\")\n",
        "            failed_models += 1\n",
        "\n",
        "    # 全体の統計表示\n",
        "    overall_end_time: float = time.time()\n",
        "    total_time: float = overall_end_time - overall_start_time\n",
        "    average_time: float = total_time / len(models_to_test)\n",
        "\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"全体テスト結果\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"成功: {successful_models} モデル\")\n",
        "    print(f\"失敗: {failed_models} モデル\")\n",
        "    print(f\"総実行時間: {total_time:.2f} 秒\")\n",
        "    print(f\"平均実行時間: {average_time:.2f} 秒/モデル\")\n",
        "    print(f\"完了時刻: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "\n",
        "def setup_api_key() -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Google Colab のシークレットマネージャーからAPIキーを取得し、設定します。\n",
        "\n",
        "    Returns:\n",
        "        Optional[str]: 取得に成功した場合はAPIキー、失敗した場合はNone\n",
        "\n",
        "    Raises:\n",
        "        SystemExit: APIキーの取得または設定に失敗した場合\n",
        "    \"\"\"\n",
        "    try:\n",
        "        api_key: str = userdata.get('GOOGLE_API_KEY')\n",
        "        genai.configure(api_key=api_key)\n",
        "        return api_key\n",
        "    except userdata.SecretNotFoundError:\n",
        "        print(\"シークレット 'GOOGLE_API_KEY' が見つかりません。ColabのシークレットマネージャーにAPIキーを設定してください\")\n",
        "        exit()\n",
        "    except Exception as e:\n",
        "        print(f\"APIキーの設定中にエラーが発生しました: {e}\")\n",
        "        exit()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    GOOGLE_API_KEY: Optional[str] = setup_api_key()\n",
        "\n",
        "    if GOOGLE_API_KEY is not None:\n",
        "        main()\n",
        "    else:\n",
        "        print(\"APIキーが見つからなかったため、実行されませんでした\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4-gnvDSlTS6h",
        "outputId": "3d5e3a7b-893c-4b77-d309-119ade68de84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Generative AI モデル比較テスト\n",
            "プロンプト: \"真空中でもなぜ熱は伝わるのでしょうか？簡潔に説明してください。\"\n",
            "テスト対象モデル数: 2\n",
            "開始時刻: 2025-05-23 04:38:13\n",
            "\n",
            "1/2 番目のモデルをテスト中...\n",
            "\n",
            "============================================================\n",
            "LLMモデル: gemini-2.5-flash-preview-05-20\n",
            "============================================================\n",
            "生成開始時刻: 2025-05-23 04:38:13\n",
            "生成中...\n",
            "\n",
            "生成結果:\n",
            "----------------------------------------\n",
            "真空中でも熱が伝わるのは、**熱放射（ねつほうしゃ）**によるものです。\n",
            "\n",
            "*   熱の伝わり方には「伝導」「対流」「放射」の3種類がありますが、真空中には物質がないため、「伝導」や「対流」は起こりません。\n",
            "*   しかし、**熱放射（赤外線などの電磁波）は物質を介さずにエネルギーを直接空間に放出します。**この電磁波が真空中を伝わり、別の物体に吸収されることで熱として伝わるのです。\n",
            "\n",
            "太陽の熱が地球に届くのも、この熱放射によるものです。\n",
            "----------------------------------------\n",
            "\n",
            "生成統計:\n",
            "   生成時間: 6.11 秒\n",
            "   完了時刻: 2025-05-23 04:38:19\n",
            "   プロンプトトークン数: 17\n",
            "   生成トークン数: 135\n",
            "   総トークン数: 1005\n",
            "   生成文字数: 221 文字\n",
            "   生成単語数: 6 語\n",
            "   生成速度: 36.2 文字/秒\n",
            "   ステータス: 成功\n",
            "============================================================\n",
            "\n",
            "\n",
            "2/2 番目のモデルをテスト中...\n",
            "\n",
            "============================================================\n",
            "LLMモデル: gemma-3n-e4b-it\n",
            "============================================================\n",
            "生成開始時刻: 2025-05-23 04:38:19\n",
            "生成中...\n",
            "\n",
            "生成結果:\n",
            "----------------------------------------\n",
            "真空中でも熱が伝わるのは、**熱が物質の振動によって生じるエネルギーであり、その振動が波として伝わる**ためです。\n",
            "\n",
            "*   **物質（固体、液体、気体）**：分子や原子が振動している。\n",
            "*   **真空**：物質がないが、振動はエネルギーとして伝わる。\n",
            "*   **伝播**：振動が近傍の分子を振動させ、その振動が連鎖的に伝わる。これが熱の伝わり方。\n",
            "\n",
            "熱伝導の仕組みは、物質の有無に関わらず、エネルギーの振動として伝わることに起因します。\n",
            "----------------------------------------\n",
            "\n",
            "生成統計:\n",
            "   生成時間: 2.24 秒\n",
            "   完了時刻: 2025-05-23 04:38:21\n",
            "   プロンプトトークン数: 17\n",
            "   生成トークン数: 132\n",
            "   総トークン数: 149\n",
            "   生成文字数: 221 文字\n",
            "   生成単語数: 8 語\n",
            "   生成速度: 98.8 文字/秒\n",
            "   ステータス: 成功\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "全体テスト結果\n",
            "============================================================\n",
            "成功: 2 モデル\n",
            "失敗: 0 モデル\n",
            "総実行時間: 8.35 秒\n",
            "平均実行時間: 4.17 秒/モデル\n",
            "完了時刻: 2025-05-23 04:38:21\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}